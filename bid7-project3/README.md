# bid7-project3 README/Report
Project 3 repository created for bid7


### Describe how to load and run your trained agent:
* To load and run my trained agent, simply do "python2.7 Qagent.py". The trained file it is expecting (that is already built in without having to specify the exact file) is table.txt. 
* I have included two additional files on top of table.txt (table2.txt and table_good_2.txt). Both table.txt and table_good_2.txt shows 3 frogs being trained to get to their homes. Table2.txt however only shows two frogs being trained to get home (will need more time). To use any of the files, simply renamed the file to table.txt and run python2.7 Qagent.py. 
* Whatever file is named "table.txt" will be the training data that the program will use. If table.txt does not exist, then it will generate the file itself and populate it all fields to zero (basically no training data). 
* The format of my data file is an list of 14 by 14 array. Each array index contains an dictionary of key/value pair. The keys are the actions and the value are the Q value. There are a total of 5 14x14 arrays in the list. Each array is used by a unique frog. 

### Describe how well your final agent is performing -- On average, how many frogs make it home safely per game? What is the average of the total reward the agent received per game?
* Depending on the training data that is being used, on average, 2-3 frogs make it home per game. The total reward the agent received per game depends on the path that it takes.
* I have set the reward to reach home to be at 100. Upon reaching home, it will receive 100 to be rewarded. However, if it dies trying to reach home, the reward it receives will be -1. Any "non-useful" work meaning going "left" and immediately "right" or "up" and then immediately "down" will yield a -.1 reward (punishment system). The reason for this is because the frog is wasting time by doing work and immediately undoing it. As a result, having a negative reward as punishment is best. This logic goes hand-in-hand as having a negative reward for every time the frog moves... meaning the longer it stays alive, the less reward it will receive. Instead of basing my punishment to be how long it stays alive, I decided to punish it by how useful the work is. If the work is not useful (meaning doing the work and immediately undoing it), it will be punished. With all the reward system and the punishment system, the average really depends on what path the frog takes. 

### Discuss the performance of your agent. If your agent does not work for a the full environment, describe to what extent your agent is able to exhibit any sort of learning behavior. If your agent basically works (but not perfectly), discuss what might be improved.
* My agent basically works but does not work perfectly. Since reinforcement learning is a lot about "chance" right off the bat, it is hard to make the frog learn something "quicker" since its strictly depended on luck. For example, in order for the frog to learn that moving up is the right thing to do, it must be rewarded with a positive number. However, the first positive reward it receives is after it reaches the middle. As a result, when trying to cross the road to reach the middle, there is a lot of "luck" involved. It strictly depends on how lucky you are to randomly pick an action that the frog can take that can potentially get it to reach the center since Q(s,a) basically all yields the same result when the board is first initialized. The earlier the game is, the more the luck factor is involved. Once you have some sort of training data, the luck factor can be mitigated as now you can rely more on the max of Q(s,a) rather than on luck. This whole process is repeated for each and every frog. Once a frog reached home, a new frog appears and you will need to train that frog from scratch as well. The only thing that I can think of to improve it would be time. If given more time, the frogs can learn more and the luck factor can be mitigated close to zero.

# Describe in sufficient details the choice you've made to get the learning framework going.
* As stated up above, I have a punishment system where if the frog does not do any "useful" work, it will be punished with a negative reward. The negative reward is .-1. Furthermore, I set the learning rate (which is alpha) to be 0.5. I have played around with alpha and having alpha set to 0.5 seems to work best for me. As for gamma value, I decided to leave it as 1. I don't like the idea of discounts. Whenever you try to find an action, you discount the max reward you could potentially get. I believe that whenever there is a good state, it should be consistently good. I shouldn't need to discount it. The reward I get from that state should be the reward I receive rather than the discounted reward. That is the reason why I decided to leave it as 1. At first, there is a 5% chance that the frog will pick a random action. Which means that there is a 95% chance that the frog will pick an action that it is determined by max Q(s',a'). However, once the frog has reached home consistently, I have taken out the random factor completely. In order to determine consistently, I give the frog 5 death chances to reach back home. If it fails to reach back home 5 times in a row, then the random factor will be introduced again. However, if the frog reached back home at least once out of the 5 tries, then the random factor will be eliminated. This will remain the case for all stages. Meaning even though the frog has reached back home once out of the 5 tries, I will continue to monitor that frog and still apply the rule. If it die 5 times consecutively, we will introduce random factor. This is applied to all frogs. To sum this up, chance node is introduced for all frogs. However once it reached back home, we will eliminate chance node for that frog altogether unless it dies 5 times consecutively which then we will introduce chance node for that frog again until it reached back home again. I also introduced a time system for each frog. Each frog has only 30 seconds to either get home or die. If they don't then we will reset the board. The reason for this is because for most games, there are a time factor involved. However, if the agent takes too long, then the agent is doing either no work at all (looping infinitely) or doing too much work (traveling the entire board but still not reaching home). As a result, I have decided that if they last longer than 30 seconds, we will introduce chance node and increase the chance value. That way, either the chance value give it a good action and gets it home or gives it a bad action and have it killed. 
* The state action representation of my frogger is: I have a 14 by 14 array. Each array indices hold a dictionary of key value pairs. The key will be the action and the value will be the Q-value. I have decided to get rid of no-op all together as I believe a frog staying still isn't really doing "work" (professor Rebecca approved of this). While the 14 by 14 array represents state, the dictionary inside each of the state represents action. However, some state has different action than another. For example, if you are at the very bottom, the only action you have is up, left, or right. However, if you are at the very corner, let's say the bottom left corner, then the only action you have is up or right. Each frog will have their own copy of the 14 by 14 array. Which means each frog will have their own Q(s,a). Basically since I mapped the state/action to be of size 14 by 14 by 5 for all 5 frogs, the state action pair is significantly smaller than trying to map out all the pixels which will not fit in memory. Since we are using the table based approach, its not going to affect the performance of the frog too much as looking up an action for a specific value is constant time. Since we are doing this relatively fast and repeatedly, a good lookup is best. The reason for a 14 by 14 array is because there are a total of 14 rows and 14 columns. Each action the frog does will take it to either a different row or a different column. As a result, it is obvious that the states could be mapped out in 14 by 14. Each frogs need to be trained differently because the cars, logs, and turtles are not static. One path that may work for one frog probably will not work for the other frog (because different cars, logs, and turtle position). Because each frogs need to be trained differently, they will need to have a different 14 by 14 array table. 

# Overall, how long did it take you to train the frog? What do you think you'd need to do to make training more efficient?
* Overall, it took on average 5-12 hours to train all 3 frogs to reach back home. Again, the time strictly depends on luck since in order to really get the frog going, it needs to first reach the middle by pure luck. Once it has a positive reward, then everything else becomes slightly easier. This first part involves heavily on luck. If I have unlimited electricity or cores, I would run multiple frogger at the same time and hope to get all 5 frogs across. One thing I can do to make training more efficient is to come up with a function that introduce chance. Instead of giving the frog 5 deaths and re-introducing chance at a fixed rate, I would have a function where the more time they reached home, the lesser the chance will be. The more time they die, the higher the chance rate will be. By doing so, I think I can speed up the training by a tiny bit. 

# If you used any additional tools or resources, give proper citations for them. If you've discussed the problem with other people, let us know the extent of your collaboration.
* I have discussed the hw assignment with Yunkai Tang. He discussed his approach (feature-based) and I discussed my approach (table-based). We came to an agreement that table-based might be a little bit easier since it seems like less work needs to be done up front (don't need to find features) but it will take longer to get the frog going and doing "useful" work. 

# Additional things that I have done/tried (Timeline/Choices):
* At first, I gave the frogs the incentive to move up by giving the "up" action a positive reward. This proved to work actually... really well. However, when I spoke to professor Hwa about my approach, she said that I should not have done that. The reason being is that WE know that going up is the best thing but the FROG does not. We cannot give the frog the incentive to move up based on what WE know. The frog needs to learn that. For example, what happens if instead of the home being up at the top? What happens if the home is at the bottom and the frog starts at the top? Then we would have to modify the reward such that the frog has the incentive to move down rather than up. As a result, the strategy that I have used did not work. However, since the frog was able to figure out a path home after given the incentive to move up, I know that my Q-learning code does work. All I need to do is find a way to get the frog to learn the path.
* What I did afterwards was to change the up reward value back to zero and observe what happens. I notice that the frog will stay at the bottom going left and right, left and right for an infinite amount of times. Since it is at the very bottom row, no cars will come and hit the frog thus the frog will never die and never receive a negative reward. That is when I decided to introduce the punishment system. Since the frog isn't doing useful work, I decide to give it a negative one. Since professor Hwa has justified that we can give the frogs negative reward for how long they stay alive, then I could take the same argument and argue that I can give the frog negative reward for how useful the frog has done its work. For example, since we go left and immediately right, then what is the point of going left in the first place? The work it has done is not useful and rather a waste of time. As a result, it should deserve to be punished. Implementing this was relatively easy and proved to be effective as now the frog no longer stays at the bottom and instead ventures out into the open world of cars, turtles, logs, and alligators. 
* This seems to be working until the point where it repeatedly dies the same way even when no useless work is being done. After a close observation, when it reached Q(s,a), the value there is negative one. (The fact that it reached there means that Q(s,a) was the best state from before). The reward at Q(s,a) was -1 and max of Q(s',a') was zero. What this means is that Qsample will be -1. Because Qsample is -1 and old Q(s,a) is -1, updating Q(s,a) will also be -1 (because Old Q(s,a) + (Qsample - Old Q(s,a)) = -1 + -1 - -1 = -1. Which means the old Q(s,a) is the same as the new Q(s,a) that originally had gotten the frog to die. Since the value is never changing, the frog will repeatedly die the same way. A way around this would be to introduce the chance node. That there will be a chance that the frog will pick a random action rather than the max Q(s',a'). This way, the frog will have a chance of not reaching the state that it will die in. Implementing this solves the issue. 
* However, a new issue came about. What happens if there is a path home? Then the chance will mess the frog up and go a different path causing it to die instead of reaching home safely. To solve this, I was able to come up with a way to eliminate chance node based on certain requirement and reintroduce it based on certain requirement. The requirement is as stated above. Implementing this solves this issue.
* But what happens if let's say it gets home the first time but on the second time, it decides to stay at the bottom and not die? Since we have eliminated the chance node, there will be no chance of forcing the frog outside and having it get hit by a car or receiving a negative reward. That is when I decided to introduce a time system to only allow the frog 30 seconds of time to get home. If it fails in that 30 seconds of time, the board will be reset and the chance factor will be reintroduced for that frog. 
* I had to play around with the % chance of it picking a random action rather than picking the max. I notice that the situation I had before implementing the chance node wasn't quite frequent. As a result, I can get away with having my % chance to be small. I have set it to 5% chance that it will pick a random action. This seems to be quite effective. 

# Analysis
* Using Table-Based approach, the difficulty is to represent it in a way that it will fit the memory. Feature-based approach prevents that issue. Table-Based is more of memorization... memorizing all the states and all the actions. Having a big table means that you are accounting for a lot of different states. The issue with having a big table is that you might not have enough time to explore all the state,actions. Having a small table means that you are shrinking the possible states into a small set. The issue with this is that because the table is smaller, one state might represent two different things. 
* Getting the frogs back home is difficult. It involves a lot of "luck" as stated up above in topic 3. However, another factor could be maybe the state action representation is too big and that it takes a long time to train one frog to get back home because it wasnt able to explore all the needed actions. 
* Since I decided on using table-based approach, I have two options... Either using large table or small table. I decided to go with a smaller table because there is one other factor. Since each frog has their own table, I can base the table off of time. The cars are always moving but after reset, it always start out at the same place and travels at the same speed. Let's say that at Time T=1, the coordinates die in (1,2), I know that there is a car at (1,2). When I respawn and at Time T=0, I know that at Time T=1, the car will be at (1,2) so I cannot move to (1,2). Instead, I can move somewhere else and try to move up from that point on. There is this element of "time" hidden in my table state that I'm not directly representing in my table. This is the reason for why I can get away with having a smaller table because my table is also based on the hidden "time" feature. This allowed me to distinguish similar states. 
* If I have decided to go with feature approach, coming up with feature would have been challenging. I would have to figure out the weights of the features. If I have too much feature, that may cause an issue in deciding what the next action move would be. If I have too few features, that may result in more death. After considering this and weighing my options carefully, I decided to go with the table based approach mainly because I dont have to worry about finding the "middle ground" for feature based approach. 
* One other analysis is that even if I went with a bigger table which represents all states in terms of pixels, I could potentially be able to use it by having multiple files. I am confident enough that if I have 1 large table that is only used for 1 frog, it can fit into memory. The problem would be fitting 5 large tables in memory for 5 frogs. In the back of my mind, I think I can potentially avoid this (if I were to use large table) by writing to a file for each of the frog. If frog 1 crossed, write to file 1 frog 1's table and read in frog 2's table. If frog 2 crossed, write to frog 2's file the current frog 2's table and read in frog 3's table. I will repeat this until I am able to get all 5 frogs to cross. The tradeoff would be that I would be able to more accurately map out a path for the frog at the cost of speed (because reading and writing is slow). Although this method gets rid of the memory constraint issue, it does not necessarily solve the other issue which I have described up above. Could the table be too big and that there would be a lot more time needed to explore each state to determine which one is good vs which one is bad? Could the table be too big such that given the amount of time needed to do the project, it is not possible to get all 5 frogs across? These issues will still remain.
